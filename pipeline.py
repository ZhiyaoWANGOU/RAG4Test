#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
pipeline.py
ä¿®æ­£ç‰ˆï¼šä½¿ç”¨æ­£ç¡®çš„ cosine similarity è®¡ç®—é€»è¾‘
"""

import json, math, csv
import numpy as np
import chromadb
from chromadb.utils import embedding_functions

# ====== å‚æ•°é…ç½® ======
DB_PATH = "./kb_index"
LAYER1_NAME = "firefox_kb"
LAYER2_NAME = "firefox_bugs"
USER_FEEDBACK_PATH = "user_feedback.json"
OUTPUT_CSV = "pipeline_results.csv"
TOP_K = 10
THRESHOLD_LOW = 0.75   # è‹¥ç›¸ä¼¼åº¦å‡ä½äºæ­¤é˜ˆå€¼ -> æ¿€æ´» Layer3
BETA_BASE = 2.0        # æ¸©åº¦å› å­åŸºå€¼

# ====== åˆå§‹åŒ–å®¢æˆ·ç«¯ ======
print("ğŸš€ Starting cosine-similarity pipeline...\n")
client = chromadb.PersistentClient(path=DB_PATH)
collections = [c.name for c in client.list_collections()]
print("âœ… Found collections:", collections)

layer1 = client.get_collection(LAYER1_NAME)
layer2 = client.get_collection(LAYER2_NAME)

# ====== è¯»å– feedback ======
with open(USER_FEEDBACK_PATH, "r", encoding="utf-8") as f:
    feedbacks = json.load(f)
print(f"ğŸ“¥ Loaded {len(feedbacks)} feedback entries.\n")

# ====== å‡½æ•°å®šä¹‰ ======
def compute_cosine_similarity(distances):
    """Chroma è¿”å›çš„ cosine è·ç¦» = 1 - cosine_similarity"""
    return [1 - d for d in distances]

def adaptive_weights(s1, s2, alpha1=0.5, alpha2=0.5, beta_base=2.0):
    """æ ¹æ®ç›¸ä¼¼åº¦å·®å¼‚è°ƒæ•´æ¸©åº¦å¹¶è®¡ç®—æƒé‡"""
    diff = abs(s1 - s2)
    beta = beta_base * diff * 5     # æ”¾å¤§å› å­ *5 è®© Î² è½åœ¨ 0.1~1.0 åŒºé—´
    w1 = alpha1 * math.exp(beta * s1)
    w2 = alpha2 * math.exp(beta * s2)
    z = w1 + w2
    return w1/z, w2/z, beta


# ====== ä¸»å¾ªç¯ ======
results = []
layer3_count = 0

for fb in feedbacks:
    qtext = f"{fb['title']} {fb['summary']}"
    fid = fb["id"]

    # --- Layer1 ---
    res1 = layer1.query(query_texts=[qtext], n_results=TOP_K)
    dists1 = res1["distances"][0]
    sim1 = compute_cosine_similarity(dists1)
    s1 = max(sim1) if sim1 else 0.0

    # --- Layer2 ---
    res2 = layer2.query(query_texts=[qtext], n_results=TOP_K)
    dists2 = res2["distances"][0]
    sim2 = compute_cosine_similarity(dists2)
    s2 = max(sim2) if sim2 else 0.0

    # --- æƒé‡è®¡ç®— ---
    w1, w2, beta = adaptive_weights(s1, s2, beta_base=BETA_BASE)

    # --- Layer3 åˆ¤æ–­ ---
    need_layer3 = s1 < THRESHOLD_LOW and s2 < THRESHOLD_LOW
    if need_layer3:
        layer3_count += 1

    # --- ä¿å­˜ç»“æœ ---
    results.append({
        "id": fid,
        "w1": round(w1, 4),
        "w2": round(w2, 4),
        "s1": round(s1, 4),
        "s2": round(s2, 4),
        "beta": round(beta, 4),
        "activate_layer3": int(need_layer3)
    })

# ====== å†™å‡º CSV ======
with open(OUTPUT_CSV, "w", newline="", encoding="utf-8") as f:
    writer = csv.DictWriter(f, fieldnames=results[0].keys())
    writer.writeheader()
    writer.writerows(results)

print(f"âœ… Done. Saved results to {OUTPUT_CSV}")
print(f"ğŸ“Š Total processed: {len(results)}")
print(f"ğŸš€ Layer3 triggered: {layer3_count} times.\n")
